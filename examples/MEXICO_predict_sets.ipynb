{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrative Examples of Library Function Usage\n",
    "\n",
    "This notebook is designed for Hygia library users and provides examples of utilizing the main functions in the library. It is one of the resources offered by the Hygia community to support new users. For further information, please visit our documentation at https://hygia-org.github.io/hygia/.\n",
    "\n",
    "The example pipeline demonstrated in this notebook covers the following steps: importing dependencies, loading the model, pre-processing the data (e.g., concatenating and creating new columns), using the prediction and model functions, and finally saving the model results.\n",
    "\n",
    "## Imports and classes instanciations\n",
    "\n",
    " To take advantage of the library's functions and proceed with the pipeline, you will first need to import the Pandas and Hygia libraries.\n",
    "\n",
    "As a starting point, when first using the library, it is recommended to initialize the pre-processing and feature engineering classes. This will set the foundation for selecting the desired model stored in the .pkl format in the folder (/data/models/).\n",
    "\n",
    "Before utilizing the library functions, it is important to familiarize yourself with the pre-processing and feature engineering classes, which play a crucial role in the data preparation process. Once you have a clear understanding of these classes, you can then proceed to select the model that best fits your needs from the available options stored in the folder (/data/models/). With the right model selected, you can then proceed to execute the pipeline and achieve the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning feature engineering with configs below...\u001b[37m\n",
      "\u001b[1mlanguage -> \u001b[22mes\n",
      "\u001b[1mdimensions -> \u001b[22m25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hygia as hg\n",
    "\n",
    "# Chose your model based on the configs sets below\n",
    "set_0 = {\n",
    "    'set_name': 'rforest_ksmash_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': True,\n",
    "    'model_output': 'RandomForest_Ksmash_Word_Embedding_Regex_Enrichments_Normalization.pkl',\n",
    "}\n",
    "set_1 = {\n",
    "    'set_name': 'rforest_ksmash_regex_normal',\n",
    "    'ignore_word_embedding': True,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "set_2 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "set_3 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_bigram_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'ignore_repeated_bigram_ratio': False,\n",
    "    'ignore_unique_char_ratio': True,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Bigram_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "set_4 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_bigram_unique_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'ignore_repeated_bigram_ratio': False,\n",
    "    'ignore_unique_char_ratio': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Bigram_Unique_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "chosen_set = set_3\n",
    "\n",
    "pre_process_data = hg.PreProcessData(country=\"MEXICO\")\n",
    "augment_data = hg.AugmentData(country=\"MEXICO\")\n",
    "feature_engineering = hg.FeatureEngineering(country=\"MEXICO\",\n",
    "                                            ignore_word_embedding=chosen_set.get('ignore_word_embedding'),\n",
    "                                            ignore_shannon_entropy=chosen_set.get('ignore_shannon_entropy'),\n",
    "                                            ignore_repeated_bigram_ratio=chosen_set.get('ignore_repeated_bigram_ratio'),\n",
    "                                            ignore_unique_char_ratio=chosen_set.get('ignore_unique_char_ratio'),\n",
    "                                            )\n",
    "rf_model = hg.RandomForestModel(f\"../data/models/{chosen_set['model_output']}\",\n",
    "                                normalization_absolutes_file=f\"../data/models/normalization_absolutes_{chosen_set['set_name']}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To showcase the capabilities of the Hygia library, we have provided a small sample of context-free data. However, the library is designed to handle a wide range of data types and can be customized to meet the unique needs of different datasets.\n",
    "\n",
    "We have leveraged the pandas library to read in the sample data, which is stored in a .csv file format. The following code block provides an example of how to import the pandas library and read in the sample data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/tmp/AI_LATA_ADDRESS_MEX_modificado.csv'\n",
    "df = pd.read_csv(file_path, sep='Â¨', nrows=None, engine='python')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Data with context validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = augment_data.augment_data(df, zipcode_column_name='ZIP_CODE_L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns\n",
    "\n",
    "The Hygia library is designed to meet the needs of data scientists, and as such, it generates new columns in the data provided to better facilitate the data analysis process. This helps users keep track of the pre-processing steps taken on the data and the features generated. Two distinct types of columns are generated:\n",
    "\n",
    "1. Concatenate address\n",
    "2. All features columns:\n",
    "    - Key Smash\n",
    "    - Regex\n",
    "    - Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliases indified: \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2 -> \u001b[22m['STREET_ADDRESS_1', 'STREET_ADDRESS_2']\n",
      "handle null values in the column \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2\u001b[22m\n",
      "extract features from -> concat_STREET_ADDRESS_1_STREET_ADDRESS_2\n"
     ]
    }
   ],
   "source": [
    "concatened_column_name = 'concat_STREET_ADDRESS_1_STREET_ADDRESS_2'\n",
    "df = pre_process_data.pre_process_data(df, ['STREET_ADDRESS_1', 'STREET_ADDRESS_2'], concatened_column_name)\n",
    "df = feature_engineering.extract_features(df, concatened_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check new columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_ks_count_sequence_squared_vowels_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_consonants_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_average_of_char_count_squared_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_0_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_1_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_2_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_3_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_4_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_5_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_6_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_7_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_8_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_9_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_10_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_11_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_12_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_13_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_14_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_15_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_16_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_17_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_18_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_19_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_20_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_21_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_22_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_23_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_24_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_context_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_dell_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_test_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_numbers_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_email_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_url_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_date_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_is_substring_of_column_name_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_one_char_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_white_spaces_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_empty_concat_STREET_ADDRESS_1_STREET_ADDRESS_2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_columns = [col for col in df if col.startswith('feature_ks') or col.startswith('feature_we') or col.startswith('feature_re')]\n",
    "model_features_columns = all_features_columns\n",
    "model_features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using pre-trained model\n",
    "\n",
    "This notebook showcases the utilization of a pre-trained model and its demonstration through prediction with the help of the pandas library. This serves as an example of how the Hygia library can be employed to perform predictions on your data, providing insight and generating new information based on the data at hand. The notebook also highlights the versatility of the Hygia library as it can be used in conjunction with other libraries such as pandas, further expanding its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning model...\u001b[37m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    2512460\n",
       "1.0       7836\n",
       "Name: prediction_is_key_smash, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prediction_is_key_smash'] = rf_model.predict(df[model_features_columns], concatened_column_name)\n",
    "df['prediction_is_key_smash'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predicted data\n",
    "\n",
    "Por fim um exemplo de como salvar os dados e resultados do modelo armazenado no campo prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['prediction_is_key_smash'] == 1][[concatened_column_name, 'prediction_is_key_smash']] \\\n",
    "    .drop_duplicates(subset=[concatened_column_name]) \\\n",
    "    .to_csv(f\"../data/tmp/prediction_{chosen_set['set_name']}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "acd904f7927719ac3bd428a31e6feadbc6c298bbba280a82d6227cca902ecf8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
