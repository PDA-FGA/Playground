{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and classes instanciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning feature engineering with configs below...\u001b[37m\n",
      "\u001b[1mlanguage -> \u001b[22mes\n",
      "\u001b[1mdimensions -> \u001b[22m25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hygia as hg\n",
    "\n",
    "pre_process_data = hg.PreProcessData(country=\"MEXICO\")\n",
    "augment_data = hg.AugmentData(country=\"MEXICO\")\n",
    "feature_engineering = hg.FeatureEngineering(country=\"MEXICO\")\n",
    "rf_model = hg.RandomForestModel('../data/models/RandomForest_Ksmash_Regex_Enrichments_Normalization.pkl',\n",
    "                                normalization_absolutes_file='../data/models/normalization_absolutes.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/tmp/AI_LATA_ADDRESS_MEX_modificado.csv'\n",
    "df = pd.read_csv(file_path, sep='Â¨', nrows=None, engine='python')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Data with context validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = augment_data.augment_data(df, zipcode_column_name='ZIP_CODE_L')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new columns\n",
    "\n",
    "1. Concatenate address\n",
    "2. All features columns:\n",
    "    - Key Smash\n",
    "    - Regex\n",
    "    - Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliases indified: \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2 -> \u001b[22m['STREET_ADDRESS_1', 'STREET_ADDRESS_2']\n",
      "handle null values in the column \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2\u001b[22m\n",
      "extract features from -> concat_STREET_ADDRESS_1_STREET_ADDRESS_2\n"
     ]
    }
   ],
   "source": [
    "concatened_column_name = 'concat_STREET_ADDRESS_1_STREET_ADDRESS_2'\n",
    "df = pre_process_data.pre_process_data(df, ['STREET_ADDRESS_1', 'STREET_ADDRESS_2'], concatened_column_name)\n",
    "df = feature_engineering.extract_features(df, concatened_column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check features columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_ks_count_sequence_squared_vowels_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_consonants_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_average_of_char_count_squared_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_context_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_dell_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_test_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_numbers_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_email_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_url_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_date_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_is_substring_of_column_name_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_one_char_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_white_spaces_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_empty_concat_STREET_ADDRESS_1_STREET_ADDRESS_2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_columns = [col for col in df if col.startswith('feature_ks') or col.startswith('feature_we') or col.startswith('feature_re')]\n",
    "model_features_columns = [col for col in all_features_columns \\\n",
    "                        if not col.startswith('feature_we') \\\n",
    "                        and 'ratio_of_numeric_digits_squared' not in col]\n",
    "model_features_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning model...\u001b[37m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    2518008\n",
       "1.0       2288\n",
       "Name: prediction_is_key_smash, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prediction_is_key_smash'] = rf_model.predict(df[model_features_columns], concatened_column_name)\n",
    "df['prediction_is_key_smash'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['prediction_is_key_smash'] == 1][[concatened_column_name, 'prediction_is_key_smash']] \\\n",
    "    .drop_duplicates(subset=[concatened_column_name]) \\\n",
    "    .to_csv(f'../data/tmp/prediction_rf_ks_we_regex_enrich_normal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acd904f7927719ac3bd428a31e6feadbc6c298bbba280a82d6227cca902ecf8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
