{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de uso para treinar o modelo\n",
    "\n",
    "Welcome to the Hygia Boilerplate! This resource is designed to help data scientists understand and utilize the full capabilities of the Hygia library. The Hygia library provides a comprehensive suite of tools for pre-processing, feature engineering, model training, and prediction. By using this boilerplate, you will gain a deeper understanding of how to effectively use the library to perform various tasks in the data science pipeline.\n",
    "\n",
    "Starting with pre-processing, the Hygia library provides functions for cleaning and transforming your data. This is an important step in preparing your data for analysis and modeling. The library also includes functions for feature engineering, allowing you to create new features and extract insights from your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hygia as hg\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chose your model based on the configs sets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_0 = {\n",
    "    'set_name': 'rforest_ksmash_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': True,\n",
    "    'model_output': 'RandomForest_Ksmash_Word_Embedding_Regex_Enrichments_Normalization.pkl',\n",
    "}\n",
    "set_1 = {\n",
    "    'set_name': 'rforest_ksmash_regex_normal',\n",
    "    'ignore_word_embedding': True,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "set_2 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "set_3 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_bigram_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'ignore_repeated_bigram_ratio': False,\n",
    "    'ignore_unique_char_ratio': True,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Bigram_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "set_4 = {\n",
    "    'set_name': 'rforest_ksmash_shannon_bigram_unique_wembedding_regex_normal',\n",
    "    'ignore_word_embedding': False,\n",
    "    'ignore_shannon_entropy': False,\n",
    "    'ignore_repeated_bigram_ratio': False,\n",
    "    'ignore_unique_char_ratio': False,\n",
    "    'model_output': 'RandomForest_Ksmash_Shannon_Bigram_Unique_Word_Embedding_Regex_Enrichments_Normalization.pkl'\n",
    "}\n",
    "\n",
    "chosen_set = set_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes instanciations\n",
    "\n",
    "As a starting point, when first using the library, it is recommended to initialize the pre-processing, feature engineering, annotate data, and new random forest classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning feature engineering with configs below...\u001b[37m\n",
      "\u001b[1mlanguage -> \u001b[22mes\n",
      "\u001b[1mdimensions -> \u001b[22m25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_process_data = hg.PreProcessData(country=\"MEXICO\")\n",
    "augment_data = hg.AugmentData(country=\"MEXICO\")\n",
    "feature_engineering = hg.FeatureEngineering(country=\"MEXICO\",\n",
    "                                            ignore_word_embedding=chosen_set.get('ignore_word_embedding'),\n",
    "                                            ignore_shannon_entropy=chosen_set.get('ignore_shannon_entropy'),\n",
    "                                            ignore_repeated_bigram_ratio=chosen_set.get('ignore_repeated_bigram_ratio'),\n",
    "                                            ignore_unique_char_ratio=chosen_set.get('ignore_unique_char_ratio'),\n",
    "                                            )\n",
    "annotate_data = hg.AnnotateData()\n",
    "new_rf_model = hg.RandomForestModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "To showcase the capabilities of the Hygia library, we have provided a small sample of context-free data. However, the library is designed to handle a wide range of data types and can be customized to meet the unique needs of different datasets.\n",
    "\n",
    "We have leveraged the pandas library to read in the sample data, which is stored in a .csv file format. The following code block provides an example of how to import the pandas library and read in the sample data file.\n",
    "\n",
    "NOTE: Please check if the file_path matches your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/tmp/AI_LATA_ADDRESS_MEX_modificado.csv'\n",
    "df = pd.read_csv(file_path, sep='Â¨', nrows=None, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns\n",
    "\n",
    "The Hygia library is designed to meet the needs of data scientists, and as such, it generates new columns in the data provided to better facilitate the data analysis process. This helps users keep track of the pre-processing steps taken on the data and the features generated. Two distinct types of columns are generated:\n",
    "\n",
    "1. Concatenate address\n",
    "2. All features columns:\n",
    "    - Key Smash\n",
    "    - Regex\n",
    "    - Word Embedding\n",
    "\n",
    "NOTE: Please check if the columns names matches your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliases indified: \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2 -> \u001b[22m['STREET_ADDRESS_1', 'STREET_ADDRESS_2']\n",
      "handle null values in the column \u001b[1mconcat_STREET_ADDRESS_1_STREET_ADDRESS_2\u001b[22m\n",
      "extract features from -> concat_STREET_ADDRESS_1_STREET_ADDRESS_2\n"
     ]
    }
   ],
   "source": [
    "concatened_column_name = 'concat_STREET_ADDRESS_1_STREET_ADDRESS_2'\n",
    "df = pre_process_data.pre_process_data(df, ['STREET_ADDRESS_1', 'STREET_ADDRESS_2'], concatened_column_name)\n",
    "df = feature_engineering.extract_features(df, concatened_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_ks_count_sequence_squared_vowels_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_consonants_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_count_sequence_squared_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_average_of_char_count_squared_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_repeated_bigram_ratio_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_ks_unique_char_ratio_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_0_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_1_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_2_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_3_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_4_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_5_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_6_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_7_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_8_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_9_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_10_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_11_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_12_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_13_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_14_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_15_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_16_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_17_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_18_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_19_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_20_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_21_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_22_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_23_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_we_24_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_context_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_dell_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_the_word_test_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_numbers_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_special_characters_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_email_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_url_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_date_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_contains_exactly_invalid_words_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_is_substring_of_column_name_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_one_char_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_only_white_spaces_concat_STREET_ADDRESS_1_STREET_ADDRESS_2',\n",
       " 'feature_re_empty_concat_STREET_ADDRESS_1_STREET_ADDRESS_2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_columns = [col for col in df if col.startswith('feature_ks') or col.startswith('feature_we') or col.startswith('feature_re')]\n",
    "all_features_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features\n",
    "- remove word embeddings\n",
    "- remove key smash feature: ratio_of_numeric_digits_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = all_features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate data\n",
    "\n",
    "The Hygia library has a dedicated class to assist in the process of annotating data using keyboard smashing threshold. This information can then be used to improve the performance of machine learning models by providing more relevant training data. The use of the Hygia library's annotation functions is a key step in ensuring that your data is ready for analysis and can lead to more accurate and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning annotate data with configs below...\u001b[37m\n",
      "\u001b[1mthresholds -> \u001b[22m{'count_sequence_squared_vowels': ['above', 1.0], 'count_sequence_squared_consonants': ['above', 1.999], 'count_sequence_squared_special_characters': ['above', 2.2499], 'ratio_of_numeric_digits_squared': ['above', 2.9], 'average_of_char_count_squared': ['above', 2.78], 'shannon_entropy': ['below', 1.0], 'repeated_bigram_ratio': ['above', 1.7058], 'unique_char_ratio': ['below', 1.15789]}\n",
      "column -> concat_STREET_ADDRESS_1_STREET_ADDRESS_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "valid                             1337828\n",
       "key_smash                             645\n",
       "contains_email                        567\n",
       "contains_exactly_the_word_test        177\n",
       "only_special_characters               144\n",
       "contains_context_invalid_words        128\n",
       "contains_exactly_the_word_dell        125\n",
       "only_numbers                          106\n",
       "only_one_char                          14\n",
       "contains_exactly_invalid_words         10\n",
       "is_substring_of_column_name             3\n",
       "contains_date                           1\n",
       "empty                                   1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_smash_thresholds = {\n",
    "    'count_sequence_squared_vowels': ['above', 1.00],\n",
    "    'count_sequence_squared_consonants':['above',  1.999],\n",
    "    'count_sequence_squared_special_characters': ['above', 2.2499],\n",
    "    'ratio_of_numeric_digits_squared': ['above', 2.9],\n",
    "    'average_of_char_count_squared': ['above', 2.78],\n",
    "    'shannon_entropy' : ['below', 1.0],\n",
    "    'repeated_bigram_ratio' : ['above', 1.7058],\n",
    "    'unique_char_ratio' : ['below', 1.15789],\n",
    "}\n",
    "\n",
    "\n",
    "df = annotate_data.annotate_data(df, concatened_column_name, key_smash_thresholds)\n",
    "df.drop_duplicates(subset=[concatened_column_name])['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valid                             2511552\n",
       "contains_context_invalid_words       3079\n",
       "key_smash                            1472\n",
       "only_special_characters              1291\n",
       "contains_email                       1045\n",
       "contains_exactly_the_word_test        667\n",
       "contains_exactly_the_word_dell        553\n",
       "only_one_char                         287\n",
       "only_numbers                          239\n",
       "empty                                  71\n",
       "contains_exactly_invalid_words         26\n",
       "is_substring_of_column_name            12\n",
       "contains_date                           2\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: retrain model\n",
    "\n",
    "In addition to pre-processing and feature engineering, the Hygia library provides tools for training and retraining models. You can use the available models, or train your own using the functions provided. Once you have trained your model, you can use the prediction function to make predictions based on your data. Finally, the library includes functions for saving your model, so that you can use it again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtranning model...\u001b[37m\n",
      "\u001b[32mdone\u001b[37m\n",
      "\u001b[33mget model score...\u001b[37m\n",
      "\u001b[1maccuracy -> \u001b[22m0.9857142857142858\n",
      "\u001b[1mprecision -> \u001b[22m0.967741935483871\n",
      "\u001b[1mrecall -> \u001b[22m0.972972972972973\n",
      "\u001b[1mf1 -> \u001b[22m0.9703504043126685\n"
     ]
    }
   ],
   "source": [
    "scores = new_rf_model.train_and_get_scores(df, concatened_column_name, selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using pre-trained model\n",
    "\n",
    "After retraining the model you can make the prediction and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrunning model...\u001b[37m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    1337014\n",
       "1.0       2735\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prediction'] = new_rf_model.predict(df[selected_features], concatened_column_name)\n",
    "df.drop_duplicates(subset=[concatened_column_name])['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mexporting model and normalization absolutes...\u001b[37m\n"
     ]
    }
   ],
   "source": [
    "new_rf_model.export_model(f\"../data/models/{chosen_set['model_output']}\",\n",
    "                          f\"../data/models/normalization_absolutes_{chosen_set['set_name']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['prediction'] == 1][[concatened_column_name, 'target', 'prediction']] \\\n",
    "    .drop_duplicates(subset=[concatened_column_name]) \\\n",
    "    .to_csv(f\"../data/tmp/{time.strftime('%Y%m%d-%H%M%S')}prediction_{chosen_set['set_name']}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope that this boilerplate provides you with a clear understanding of the capabilities of the Hygia library and inspires you to explore its full potential. With its comprehensive suite of tools, the Hygia library is a valuable resource for any data scientist looking to streamline their workflow and perform high-quality data analysis and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "acd904f7927719ac3bd428a31e6feadbc6c298bbba280a82d6227cca902ecf8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
